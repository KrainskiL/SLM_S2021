{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__More tutorials available on https://github.com/TuringLang/TuringTutorials__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on Bayesian models and their inference methods, check **I Probabilistic Reasoning** Chapter from Mykel J. Kochenderfer, Tim A. Wheeler, And Kyle H. Wray (2022), [Algorithms for Decision Making](https://algorithmsbook.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Coin flipping model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ]add Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Turing, StatsPlots, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "@model coinflip(y) = begin\n",
    "    # Prior distribution of heads.\n",
    "    p ~ Beta(1, 1) #equal to U(0,1)\n",
    "\n",
    "    N = length(y)\n",
    "    for n in 1:N\n",
    "        # Heads or tails are drawn from a Bernoulli distribution.\n",
    "        y[n] ~ Bernoulli(p)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip coin 10 times\n",
    "Random.seed!(42)\n",
    "data = rand(Bernoulli(0.5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for Hamiltonian Monte Carlo (HMC) sampler.\n",
    "iterations = 1000\n",
    "ϵ = 0.05\n",
    "τ = 10\n",
    "\n",
    "chain = sample(coinflip(data), HMC(ϵ, τ), iterations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(chain[:p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip coin 100 times\n",
    "Random.seed!(42);\n",
    "data = rand(Bernoulli(0.5), 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = sample(coinflip(data), HMC(ϵ, τ), iterations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram(chain[:p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Distributions, RDatasets, MCMCChains, Plots, Random, Turing, StatsPlots\n",
    "using StatsFuns: logistic\n",
    "\n",
    "Random.seed!(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \"Default\" dataset.\n",
    "data = RDatasets.dataset(\"ISLR\", \"Default\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation - convert Yes to 1 and No to 0\n",
    "data[!,:Student] = Int.(data.Student .== \"Yes\")\n",
    "data[!,:Default] = Int.(data.Default .== \"Yes\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first(data, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to train and test dataset.\n",
    "function split_data(df, at = 0.70)\n",
    "    (r, _) = size(df)\n",
    "    index = Int(round(r * at))\n",
    "    train = df[1:index, :]\n",
    "    test  = df[(index+1):end, :]\n",
    "    return train, test\n",
    "end\n",
    "\n",
    "# Split in main dataset in 5/95 ratio.\n",
    "train, test = split_data(data, 0.05);\n",
    "\n",
    "# Creating label vectors\n",
    "train_label = train[!,:Default];\n",
    "test_label = test[!,:Default];\n",
    "\n",
    "# Extracting predictors (independent variables)\n",
    "train = train[!,[:Student, :Balance, :Income]];\n",
    "test = test[!,[:Student, :Balance, :Income]];\n",
    "train = Matrix(train);\n",
    "test = Matrix(test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale data\n",
    "train = (train .- mean(train, dims=1)) ./ std(train, dims=1);\n",
    "test = (test .- mean(test, dims=1)) ./ std(test, dims=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Declaration \n",
    "\n",
    "`logistic_regression` takes four arguments:\n",
    "\n",
    "- `x` is our set of independent variables;\n",
    "- `y` is the element we want to predict;\n",
    "- `n` is the number of observations;\n",
    "- `σ` is the standard deviation for priors.\n",
    "\n",
    "Within the model, we create four coefficients (`intercept`, `student`, `balance`, and `income`) and assign a prior of normally distributed with means of zero and standard deviations of `σ`.\n",
    "\n",
    "The `for` block creates a variable `v` which is the logistic function. We then observe the likelihood of calculating `v` given the actual label, `y[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian logistic regression (LR)\n",
    "@model logistic_regression(x, y, n, σ) = begin\n",
    "    intercept ~ Normal(0, σ)\n",
    "\n",
    "    student ~ Normal(0, σ)\n",
    "    balance ~ Normal(0, σ)\n",
    "    income  ~ Normal(0, σ)\n",
    "\n",
    "    for i = 1:n\n",
    "        v = logistic(intercept + student*x[i, 1] + balance*x[i,2] + income*x[i,3])\n",
    "        y[i] ~ Bernoulli(v)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the number of observations.\n",
    "n, _ = size(train)\n",
    "\n",
    "# Sample using HMC.\n",
    "chain = mapreduce(c -> sample(logistic_regression(train, train_label, n, 1), HMC(0.05, 10), 1500),\n",
    "    chainscat,\n",
    "    1:3\n",
    ")\n",
    "\n",
    "#Sampled parameters characteristics\n",
    "describe(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = [:student, :balance, :income]\n",
    "\n",
    "# `Corner` function from MCMCChains shows the distributions parameters of logistic regression\n",
    "corner(chain, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function prediction(x::Matrix, chain, threshold)\n",
    "    # Pull the means from each parameter's sampled values in the chain.\n",
    "    intercept = mean(chain[:intercept])\n",
    "    student = mean(chain[:student])\n",
    "    balance = mean(chain[:balance])\n",
    "    income = mean(chain[:income])\n",
    "\n",
    "    # Retrieve the number of rows.\n",
    "    n, _ = size(x)\n",
    "\n",
    "    # Generate a vector to store our predictions.\n",
    "    v = Vector{Float64}(undef, n)\n",
    "\n",
    "    # Calculate the logistic function for each element in the test set.\n",
    "    for i in 1:n\n",
    "        num = logistic(intercept .+ student * x[i,1] + balance * x[i,2] + income * x[i,3])\n",
    "        if num >= threshold\n",
    "            v[i] = 1\n",
    "        else\n",
    "            v[i] = 0\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prediction threshold\n",
    "threshold = 0.10\n",
    "\n",
    "# Make the predictions\n",
    "predictions = prediction(test, chain, threshold)\n",
    "\n",
    "# Calculate MSE for test set\n",
    "loss = sum((predictions - test_label) .^ 2) / length(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = sum(test_label)\n",
    "not_defaults = length(test_label) - defaults\n",
    "\n",
    "predicted_defaults = sum(test_label .== predictions .== 1)\n",
    "predicted_not_defaults = sum(test_label .== predictions .== 0)\n",
    "\n",
    "println(\"Defaults: $defaults\n",
    "    Predictions: $predicted_defaults\n",
    "    Percentage defaults correct $(predicted_defaults/defaults)\")\n",
    "\n",
    "println(\"Not defaults: $not_defaults\n",
    "    Predictions: $predicted_not_defaults\n",
    "    Percentage non-defaults correct $(predicted_not_defaults/not_defaults)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Neural Network is created using a combination of Turing and [Flux](https://github.com/FluxML/Flux.jl), a suite of tools machine learning. Flux is used to specify the neural network's layers and Turing to implement the probabalistic inference, with the goal of implementing a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Turing, Flux, Plots, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian neural network will be used to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points to generate.\n",
    "N = 100\n",
    "M = round(Int, N / 4)\n",
    "Random.seed!(4321)\n",
    "\n",
    "# Generate artificial data.\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \n",
    "xt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i = 1:M])\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \n",
    "append!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i = 1:M]))\n",
    "\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \n",
    "xt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i = 1:M])\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \n",
    "append!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i = 1:M]))\n",
    "\n",
    "# Store all the data for later.\n",
    "xs = [xt1s; xt0s]\n",
    "ts = [ones(2*M); zeros(2*M)]\n",
    "\n",
    "# Plot data points.\n",
    "function plot_data()\n",
    "    x1 = map(e -> e[1], xt1s)\n",
    "    y1 = map(e -> e[2], xt1s)\n",
    "    x2 = map(e -> e[1], xt0s)\n",
    "    y2 = map(e -> e[2], xt0s)\n",
    "\n",
    "    Plots.scatter(x1,y1, color=\"red\", clim = (0,1))\n",
    "    Plots.scatter!(x2, y2, color=\"blue\", clim = (0,1))\n",
    "end\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with Flux\n",
    "\n",
    "The next step is to define a feedforward neural network where parameters are expressed as distribtuions, instead of single points as with traditional neural networks. `unpack` and `nn_forward` are functions need to specify model in Turing.\n",
    "\n",
    "`unpack` takes a vector of parameters and partitions them between weights and biases. `nn_forward` constructs a neural network with the variables generated in `unpack` and returns a prediction based on the weights provided.\n",
    "\n",
    "The `unpack` and `nn_forward` functions are explicity designed to create a neural network with two hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"320\" alt=\"nn-diagram\" src=\"https://user-images.githubusercontent.com/422990/47970321-bd172080-e038-11e8-9c6d-6c2bd790bd8a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a vector into a set of weights and biases.\n",
    "function unpack(nn_params::AbstractVector)\n",
    "    W₁ = reshape(nn_params[1:6], 3, 2);   \n",
    "    b₁ = reshape(nn_params[7:9], 3)\n",
    "    \n",
    "    W₂ = reshape(nn_params[10:15], 2, 3); \n",
    "    b₂ = reshape(nn_params[16:17], 2)\n",
    "    \n",
    "    Wₒ = reshape(nn_params[18:19], 1, 2); \n",
    "    bₒ = reshape(nn_params[20:20], 1)   \n",
    "    return W₁, b₁, W₂, b₂, Wₒ, bₒ\n",
    "end\n",
    "\n",
    "# Construct a neural network using Flux and return a predicted value.\n",
    "function nn_forward(xs, nn_params::AbstractVector)\n",
    "    W₁, b₁, W₂, b₂, Wₒ, bₒ = unpack(nn_params)\n",
    "    nn = Chain(Dense(W₁, b₁, tanh),\n",
    "               Dense(W₂, b₂, tanh),\n",
    "               Dense(Wₒ, bₒ, σ))\n",
    "    return nn(xs)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabalistic model specification below creates a `params` variable, which has 20 normally distributed variables. Each entry in the `params` vector represents weights and biases of our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regularization term and a Gaussain prior variance term.\n",
    "alpha = 0.09\n",
    "sig = sqrt(1.0 / alpha)\n",
    "\n",
    "# Specify the probabalistic model.\n",
    "@model bayes_nn(xs, ts) = begin\n",
    "    # Create the weight and bias vector.\n",
    "    nn_params ~ MvNormal(zeros(20), sig .* ones(20))\n",
    "    \n",
    "    # Calculate predictions for the inputs given the weights\n",
    "    # and biases in theta.\n",
    "    preds = nn_forward(xs, nn_params)\n",
    "    \n",
    "    # Observe each prediction.\n",
    "    for i = 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference.\n",
    "N = 5000\n",
    "ch = sample(bayes_nn(hcat(xs...), ts), HMC(0.05, 4),N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization\n",
    "\n",
    "[MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) can be used to classify our population by using the set of weights that provided the highest log posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract parameter values\n",
    "theta = hcat([ch[Symbol(\"nn_params[\"*string(i)*\"]\")] for i in 1:20]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plot_data()\n",
    "\n",
    "# Find the index that provided the highest log posterior in the chain.\n",
    "_, i = findmax(ch[:lp])\n",
    "\n",
    "# Extract the max row value from i.\n",
    "i = i.I[1]\n",
    "\n",
    "# Plot the posterior distribution with a contour plot.\n",
    "x_range = collect(range(-6,stop=6,length=25))\n",
    "y_range = collect(range(-6,stop=6,length=25))\n",
    "Z = [nn_forward([x, y], theta[i, :])[1] for x=x_range, y=y_range]\n",
    "contour!(x_range, y_range, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the average predicted value across multiple weights.\n",
    "function nn_predict(x, theta, num)\n",
    "    mean([nn_forward(x, theta[i,:])[1] for i in 1:10:num])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average prediction.\n",
    "plot_data()\n",
    "\n",
    "n_end = 1500\n",
    "x_range = collect(range(-6,stop=6,length=25))\n",
    "y_range = collect(range(-6,stop=6,length=25))\n",
    "Z = [nn_predict([x, y], theta, n_end)[1] for x=x_range, y=y_range]\n",
    "contour!(x_range, y_range, Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
